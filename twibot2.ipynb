{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79d9868-b6c2-44b5-952e-fe4a7c8de6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab1eeed8-a53c-421f-bcc3-b1bcdbc2b719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.902546e+09</td>\n",
       "      <td>2016-09-01 01:06:22+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>22</td>\n",
       "      <td>73</td>\n",
       "      <td>And now, the time has come for us to sign off....</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.902546e+09</td>\n",
       "      <td>2016-08-31 23:56:32+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Meanwhile back in Yemen, reports continue to f...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.919637e+09</td>\n",
       "      <td>2022-02-20 10:46:50+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mudavadi is telling Kenyans that Hon Raila is ...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.919637e+09</td>\n",
       "      <td>2022-02-18 17:43:39+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Kenyans have already chosen Raila as the best ...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.919637e+09</td>\n",
       "      <td>2022-02-18 17:39:40+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr DP Ruto resign now!!!. Why becoming a doubl...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>1.624546e+07</td>\n",
       "      <td>2022-02-22 02:55:08+00:00</td>\n",
       "      <td>16245462.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...and no, I refuse to hot link these shit sta...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>4.552087e+07</td>\n",
       "      <td>2022-02-21 05:52:44+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I need Cassie in a horror movie</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>4.552087e+07</td>\n",
       "      <td>2022-02-21 05:41:01+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>East highland high a mess</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>4.552087e+07</td>\n",
       "      <td>2022-02-21 04:17:39+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay but luna lowkey eating kat up</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>4.552087e+07</td>\n",
       "      <td>2022-02-21 04:14:34+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Team lebron. Now for euphoria</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          author_id                 created_at  in_reply_to_user_id  \\\n",
       "0      2.902546e+09  2016-09-01 01:06:22+00:00                  NaN   \n",
       "1      2.902546e+09  2016-08-31 23:56:32+00:00                  NaN   \n",
       "2      2.919637e+09  2022-02-20 10:46:50+00:00                  NaN   \n",
       "3      2.919637e+09  2022-02-18 17:43:39+00:00                  NaN   \n",
       "4      2.919637e+09  2022-02-18 17:39:40+00:00                  NaN   \n",
       "...             ...                        ...                  ...   \n",
       "29995  1.624546e+07  2022-02-22 02:55:08+00:00           16245462.0   \n",
       "29996  4.552087e+07  2022-02-21 05:52:44+00:00                  NaN   \n",
       "29997  4.552087e+07  2022-02-21 05:41:01+00:00                  NaN   \n",
       "29998  4.552087e+07  2022-02-21 04:17:39+00:00                  NaN   \n",
       "29999  4.552087e+07  2022-02-21 04:14:34+00:00                  NaN   \n",
       "\n",
       "       possibly_sensitive  retweet_count  like_count  \\\n",
       "0                   False             22          73   \n",
       "1                   False              6           7   \n",
       "2                   False              0           1   \n",
       "3                   False              0           1   \n",
       "4                   False              0           0   \n",
       "...                   ...            ...         ...   \n",
       "29995               False              1           1   \n",
       "29996               False              0           0   \n",
       "29997               False              0           1   \n",
       "29998               False              0           0   \n",
       "29999               False              0           0   \n",
       "\n",
       "                                                    text  label  \n",
       "0      And now, the time has come for us to sign off....  human  \n",
       "1      Meanwhile back in Yemen, reports continue to f...  human  \n",
       "2      Mudavadi is telling Kenyans that Hon Raila is ...    bot  \n",
       "3      Kenyans have already chosen Raila as the best ...    bot  \n",
       "4      Mr DP Ruto resign now!!!. Why becoming a doubl...    bot  \n",
       "...                                                  ...    ...  \n",
       "29995  ...and no, I refuse to hot link these shit sta...  human  \n",
       "29996                    I need Cassie in a horror movie  human  \n",
       "29997                          East highland high a mess  human  \n",
       "29998                 Okay but luna lowkey eating kat up  human  \n",
       "29999                      Team lebron. Now for euphoria  human  \n",
       "\n",
       "[30000 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twidata.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3f72f-a67a-4477-b131-319dd085ee5c",
   "metadata": {},
   "source": [
    "# twibot_tweet0_new.csvを用いた学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9601f2b5-6f0b-4303-8488-356276b901cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データの読み込みと前処理を開始します...\n",
      "前処理が完了しました。\n",
      "BERTモデルを使用して文章の埋め込みを生成します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [07:10<00:00, 18.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [01:54<00:00, 17.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████████▎                                                                        | 1/4 [01:09<03:29, 69.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習が完了しました。学習時間: 69.78秒\n",
      "XGBoostの学習を開始します...\n",
      "XGBoost progress: 200/1500 iterations\n",
      "XGBoost progress: 400/1500 iterations\n",
      "XGBoost progress: 600/1500 iterations\n",
      "XGBoost progress: 800/1500 iterations\n",
      "XGBoost progress: 1000/1500 iterations\n",
      "XGBoost progress: 1200/1500 iterations\n",
      "XGBoost progress: 1400/1500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████▌                                                | 2/4 [02:08<02:07, 63.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost progress: 1500/1500 iterations\n",
      "XGBoostの学習が完了しました。学習時間: 59.15秒\n",
      "SVMの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████▊                        | 3/4 [02:57<00:56, 56.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMの学習が完了しました。学習時間: 48.92秒\n",
      "MLPの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [03:12<00:00, 48.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPの学習が完了しました。学習時間: 14.71秒\n",
      "アンサンブル予測を行います...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適な閾値: 0.8500000000000002\n",
      "最終的な予測結果:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.93      0.78      0.85      1796\n",
      "         bot       0.20      0.50      0.29       204\n",
      "\n",
      "    accuracy                           0.75      2000\n",
      "   macro avg       0.57      0.64      0.57      2000\n",
      "weighted avg       0.86      0.75      0.79      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "def extract_features(df):\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['has_url'] = df['text'].str.contains('http').astype(int)\n",
    "    df['hour'] = pd.to_datetime(df['created_at']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['created_at']).dt.dayofweek\n",
    "    df['is_reply'] = df['in_reply_to_user_id'].notna().astype(int)\n",
    "    df['hashtag_count'] = df['text'].apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
    "    df['mention_count'] = df['text'].apply(lambda x: len(re.findall(r'@\\w+', x)))\n",
    "    df['retweet_like_ratio'] = df['retweet_count'] / (df['like_count'] + 1)\n",
    "    df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    df['caps_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "    df['unique_words_ratio'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    df['tweet_frequency'] = df.groupby('author_id')['created_at'].transform('count')\n",
    "    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: sum(1 for c in x if c in '.,;:!?'))\n",
    "    df['emoji_count'] = df['text'].apply(lambda x: len(re.findall(r'[\\U0001F600-\\U0001F64F]', x)))\n",
    "    df['exclamation_count'] = df['text'].apply(lambda x: x.count('!'))\n",
    "    df['question_count'] = df['text'].apply(lambda x: x.count('?'))\n",
    "    df['uppercase_word_count'] = df['text'].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "    df['url_count'] = df['text'].apply(lambda x: len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)))\n",
    "    df['digit_count'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df['special_char_count'] = df['text'].apply(lambda x: len(re.findall(r'[^a-zA-Z0-9\\s]', x)))\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    df['unique_char_ratio'] = df['text'].apply(lambda x: len(set(x)) / len(x) if len(x) > 0 else 0)\n",
    "    df['tweet_hour_bin'] = pd.cut(df['hour'], bins=4, labels=[0,1,2,3])\n",
    "    df['tweet_day_bin'] = pd.cut(df['day_of_week'], bins=3, labels=[0,1,2])\n",
    "    df['text_complexity'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    return df\n",
    "\n",
    "def get_bert_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "class XGBoostProgressCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, display_interval=100, total_iterations=1000):\n",
    "        self.display_interval = display_interval\n",
    "        self.total_iterations = total_iterations\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if (epoch + 1) % self.display_interval == 0 or (epoch + 1) == self.total_iterations:\n",
    "            print(f'XGBoost progress: {epoch + 1}/{self.total_iterations} iterations')\n",
    "        return False\n",
    "\n",
    "print(\"データの読み込みと前処理を開始します...\")\n",
    "df = pd.read_csv('twibot_tweet0_new.csv')\n",
    "df = extract_features(df)\n",
    "print(\"前処理が完了しました。\")\n",
    "\n",
    "X_text = df['text']\n",
    "numeric_features = [col for col in df.columns if col not in ['text', 'label', 'author_id', 'created_at', 'in_reply_to_user_id']]\n",
    "X_numeric = df[numeric_features]\n",
    "y = df['label'].map({'human': 0, 'bot': 1})\n",
    "\n",
    "X_train_text, X_test_text, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_text, X_numeric, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=25000, ngram_range=(1, 3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric)\n",
    "\n",
    "print(\"BERTモデルを使用して文章の埋め込みを生成します...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "X_train_bert = get_bert_embeddings(X_train_text, model, tokenizer)\n",
    "X_test_bert = get_bert_embeddings(X_test_text, model, tokenizer)\n",
    "\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_numeric_scaled, X_train_bert])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_numeric_scaled, X_test_bert])\n",
    "\n",
    "variance_threshold = VarianceThreshold(threshold=0.005)\n",
    "X_train_var_selected = variance_threshold.fit_transform(X_train_combined)\n",
    "X_test_var_selected = variance_threshold.transform(X_test_combined)\n",
    "\n",
    "selector = SelectKBest(f_classif, k=\"all\")\n",
    "X_train_selected = selector.fit_transform(X_train_var_selected, y_train)\n",
    "X_test_selected = selector.transform(X_test_var_selected)\n",
    "\n",
    "# SMOTEとENNの比率を調整\n",
    "smote = SMOTE(sampling_strategy=0.8, random_state=42)\n",
    "enn = EditedNearestNeighbours() \n",
    "steps = [('over', smote), ('under', enn)]\n",
    "pipeline = ImbPipeline(steps=steps)\n",
    "\n",
    "\n",
    "\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_selected, y_train)\n",
    "# モデルのハイパーパラメータを調整\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=1500, max_depth=40, min_samples_split=5, min_samples_leaf=2, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=1500, learning_rate=0.03, max_depth=8, min_child_weight=2, subsample=0.8, colsample_bytree=0.8, random_state=42, callbacks=[XGBoostProgressCallback(display_interval=200, total_iterations=1500)], n_jobs=-1),\n",
    "    'SVM': SVC(kernel='rbf', C=50, gamma='auto', probability=True, class_weight='balanced', random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(300, 150, 75), max_iter=1500, alpha=0.0005, learning_rate='adaptive', random_state=42)\n",
    "}\n",
    "\n",
    "print(\"モデルの学習を開始します...\")\n",
    "for name, model in tqdm(models.items()):\n",
    "    print(f\"{name}の学習を開始します...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time.time()\n",
    "    print(f\"{name}の学習が完了しました。学習時間: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "print(\"アンサンブル予測を行います...\")\n",
    "y_pred_rf = models['RandomForest'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_xgb = models['XGBoost'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_svm = models['SVM'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_mlp = models['MLP'].predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# アンサンブルの重みを調整\n",
    "y_pred_ensemble = 0.35 * y_pred_rf + 0.35 * y_pred_xgb + 0.15 * y_pred_svm + 0.15 * y_pred_mlp\n",
    "y_pred_final = (y_pred_ensemble > 0.2).astype(int)  # 閾値を0.3から0.2に下げてボット検出を改善\n",
    "\n",
    "# 閾値の最適化\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    f1_scores = [f1_score(y_true, (y_pred_proba > t).astype(int)) for t in thresholds]\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "optimal_threshold = find_optimal_threshold(y_test, y_pred_ensemble)\n",
    "y_pred_final = (y_pred_ensemble > optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"最適な閾値: {optimal_threshold}\")\n",
    "print(\"最終的な予測結果:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['human', 'bot']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1860aab8-dc3e-4c47-bafe-9d94fc3908b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データの読み込みと前処理を開始します...\n",
      "前処理が完了しました。\n",
      "BERTモデルを使用して文章の埋め込みを生成します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [08:05<00:00, 16.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [01:58<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▏                                                                            | 1/5 [03:52<15:30, 232.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習が完了しました。学習時間: 232.53秒\n",
      "XGBoostの学習を開始します...\n",
      "XGBoost progress: 200/2000 iterations\n",
      "XGBoost progress: 400/2000 iterations\n",
      "XGBoost progress: 600/2000 iterations\n",
      "XGBoost progress: 800/2000 iterations\n",
      "XGBoost progress: 1000/2000 iterations\n",
      "XGBoost progress: 1200/2000 iterations\n",
      "XGBoost progress: 1400/2000 iterations\n",
      "XGBoost progress: 1600/2000 iterations\n",
      "XGBoost progress: 1800/2000 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████▍                                                         | 2/5 [07:44<11:36, 232.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost progress: 2000/2000 iterations\n",
      "XGBoostの学習が完了しました。学習時間: 231.87秒\n",
      "GradientBoostingの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████▌                                    | 3/5 [1:56:27<1:43:29, 3104.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingの学習が完了しました。学習時間: 6523.27秒\n",
      "SVMの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 4/5 [2:02:31<33:42, 2022.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMの学習が完了しました。学習時間: 363.54秒\n",
      "MLPの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [2:03:42<00:00, 1484.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPの学習が完了しました。学習時間: 71.54秒\n",
      "アンサンブル予測を行います...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適な閾値: 0.20999999999999996\n",
      "最終的な予測結果:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.92      0.97      0.95      1796\n",
      "         bot       0.54      0.30      0.39       204\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.73      0.64      0.67      2000\n",
      "weighted avg       0.89      0.90      0.89      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "def extract_features(df):\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['has_url'] = df['text'].str.contains('http').astype(int)\n",
    "    df['hour'] = pd.to_datetime(df['created_at']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['created_at']).dt.dayofweek\n",
    "    df['is_reply'] = df['in_reply_to_user_id'].notna().astype(int)\n",
    "    df['hashtag_count'] = df['text'].apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
    "    df['mention_count'] = df['text'].apply(lambda x: len(re.findall(r'@\\w+', x)))\n",
    "    df['retweet_like_ratio'] = df['retweet_count'] / (df['like_count'] + 1)\n",
    "    df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    df['caps_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "    df['unique_words_ratio'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    df['tweet_frequency'] = df.groupby('author_id')['created_at'].transform('count')\n",
    "    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: sum(1 for c in x if c in '.,;:!?'))\n",
    "    df['emoji_count'] = df['text'].apply(lambda x: len(re.findall(r'[\\U0001F600-\\U0001F64F]', x)))\n",
    "    df['exclamation_count'] = df['text'].apply(lambda x: x.count('!'))\n",
    "    df['question_count'] = df['text'].apply(lambda x: x.count('?'))\n",
    "    df['uppercase_word_count'] = df['text'].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "    df['url_count'] = df['text'].apply(lambda x: len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)))\n",
    "    df['digit_count'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df['special_char_count'] = df['text'].apply(lambda x: len(re.findall(r'[^a-zA-Z0-9\\s]', x)))\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    df['unique_char_ratio'] = df['text'].apply(lambda x: len(set(x)) / len(x) if len(x) > 0 else 0)\n",
    "    df['tweet_hour_bin'] = pd.cut(df['hour'], bins=4, labels=[0,1,2,3])\n",
    "    df['tweet_day_bin'] = pd.cut(df['day_of_week'], bins=3, labels=[0,1,2])\n",
    "    df['text_complexity'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    df['char_count'] = df['text'].apply(len)\n",
    "    df['word_density'] = df['text'].apply(lambda x: len(x.split()) / (len(x) + 1))\n",
    "    return df\n",
    "\n",
    "def get_bert_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "class XGBoostProgressCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, display_interval=100, total_iterations=1000):\n",
    "        self.display_interval = display_interval\n",
    "        self.total_iterations = total_iterations\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if (epoch + 1) % self.display_interval == 0 or (epoch + 1) == self.total_iterations:\n",
    "            print(f'XGBoost progress: {epoch + 1}/{self.total_iterations} iterations')\n",
    "        return False\n",
    "\n",
    "print(\"データの読み込みと前処理を開始します...\")\n",
    "df = pd.read_csv('twibot_tweet0_new.csv')\n",
    "df = extract_features(df)\n",
    "print(\"前処理が完了しました。\")\n",
    "\n",
    "X_text = df['text']\n",
    "numeric_features = [col for col in df.columns if col not in ['text', 'label', 'author_id', 'created_at', 'in_reply_to_user_id']]\n",
    "X_numeric = df[numeric_features]\n",
    "y = df['label'].map({'human': 0, 'bot': 1})\n",
    "\n",
    "X_train_text, X_test_text, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_text, X_numeric, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1, 3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric)\n",
    "\n",
    "print(\"BERTモデルを使用して文章の埋め込みを生成します...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "X_train_bert = get_bert_embeddings(X_train_text, model, tokenizer)\n",
    "X_test_bert = get_bert_embeddings(X_test_text, model, tokenizer)\n",
    "\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_numeric_scaled, X_train_bert])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_numeric_scaled, X_test_bert])\n",
    "\n",
    "variance_threshold = VarianceThreshold(threshold=0.001)\n",
    "X_train_var_selected = variance_threshold.fit_transform(X_train_combined)\n",
    "X_test_var_selected = variance_threshold.transform(X_test_combined)\n",
    "\n",
    "selector = SelectKBest(f_classif, k=\"all\")\n",
    "X_train_selected = selector.fit_transform(X_train_var_selected, y_train)\n",
    "X_test_selected = selector.transform(X_test_var_selected)\n",
    "\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "enn = EditedNearestNeighbours()\n",
    "X_smote, y_smote = smote.fit_resample(X_train_selected, y_train)\n",
    "X_train_resampled, y_train_resampled = enn.fit_resample(X_smote, y_smote)\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=2000, max_depth=50, min_samples_split=4, min_samples_leaf=2, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=2000, learning_rate=0.02, max_depth=10, min_child_weight=1, subsample=0.8, colsample_bytree=0.8, random_state=42, callbacks=[XGBoostProgressCallback(display_interval=200, total_iterations=2000)], n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=500, learning_rate=0.05, max_depth=8, min_samples_split=4, min_samples_leaf=2, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', C=100, gamma='scale', probability=True, class_weight='balanced', random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(400, 200, 100), max_iter=2000, alpha=0.0001, learning_rate='adaptive', random_state=42)\n",
    "}\n",
    "\n",
    "print(\"モデルの学習を開始します...\")\n",
    "for name, model in tqdm(models.items()):\n",
    "    print(f\"{name}の学習を開始します...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time.time()\n",
    "    print(f\"{name}の学習が完了しました。学習時間: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "print(\"アンサンブル予測を行います...\")\n",
    "y_pred_rf = models['RandomForest'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_xgb = models['XGBoost'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_gb = models['GradientBoosting'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_svm = models['SVM'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_mlp = models['MLP'].predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "y_pred_ensemble = 0.3 * y_pred_rf + 0.3 * y_pred_xgb + 0.2 * y_pred_gb + 0.1 * y_pred_svm + 0.1 * y_pred_mlp\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "    f1_scores = [f1_score(y_true, (y_pred_proba > t).astype(int)) for t in thresholds]\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "optimal_threshold = find_optimal_threshold(y_test, y_pred_ensemble)\n",
    "y_pred_final = (y_pred_ensemble > optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"最適な閾値: {optimal_threshold}\")\n",
    "print(\"最終的な予測結果:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['human', 'bot']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68027941-5849-48e9-a73e-2034312fe964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データの読み込みと前処理を開始します...\n",
      "前処理が完了しました。\n",
      "BERTモデルを使用して文章の埋め込みを生成します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [07:31<00:00, 17.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [01:43<00:00, 19.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▏                                                                            | 1/5 [01:52<07:28, 112.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習が完了しました。学習時間: 112.10秒\n",
      "XGBoostの学習を開始します...\n",
      "XGBoost progress: 250/2500 iterations\n",
      "XGBoost progress: 500/2500 iterations\n",
      "XGBoost progress: 750/2500 iterations\n",
      "XGBoost progress: 1000/2500 iterations\n",
      "XGBoost progress: 1250/2500 iterations\n",
      "XGBoost progress: 1500/2500 iterations\n",
      "XGBoost progress: 1750/2500 iterations\n",
      "XGBoost progress: 2000/2500 iterations\n",
      "XGBoost progress: 2250/2500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████▍                                                         | 2/5 [05:12<08:11, 163.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost progress: 2500/2500 iterations\n",
      "XGBoostの学習が完了しました。学習時間: 199.93秒\n",
      "GradientBoostingの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████▌                                    | 3/5 [1:44:58<1:34:05, 2822.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingの学習が完了しました。学習時間: 5986.47秒\n",
      "SVMの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 4/5 [1:45:46<28:46, 1726.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMの学習が完了しました。学習時間: 47.52秒\n",
      "MLPの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [1:46:15<00:00, 1275.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPの学習が完了しました。学習時間: 29.09秒\n",
      "アンサンブル予測を行います...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適な閾値: 0.8899999999999996\n",
      "最終的な予測結果:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.93      0.82      0.87      1796\n",
      "         bot       0.22      0.44      0.29       204\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.57      0.63      0.58      2000\n",
      "weighted avg       0.86      0.78      0.81      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "def extract_features(df):\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['has_url'] = df['text'].str.contains('http').astype(int)\n",
    "    df['hour'] = pd.to_datetime(df['created_at']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['created_at']).dt.dayofweek\n",
    "    df['is_reply'] = df['in_reply_to_user_id'].notna().astype(int)\n",
    "    df['hashtag_count'] = df['text'].apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
    "    df['mention_count'] = df['text'].apply(lambda x: len(re.findall(r'@\\w+', x)))\n",
    "    df['retweet_like_ratio'] = df['retweet_count'] / (df['like_count'] + 1)\n",
    "    df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    df['caps_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "    df['unique_words_ratio'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    df['tweet_frequency'] = df.groupby('author_id')['created_at'].transform('count')\n",
    "    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: sum(1 for c in x if c in '.,;:!?'))\n",
    "    df['emoji_count'] = df['text'].apply(lambda x: len(re.findall(r'[\\U0001F600-\\U0001F64F]', x)))\n",
    "    df['exclamation_count'] = df['text'].apply(lambda x: x.count('!'))\n",
    "    df['question_count'] = df['text'].apply(lambda x: x.count('?'))\n",
    "    df['uppercase_word_count'] = df['text'].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "    df['url_count'] = df['text'].apply(lambda x: len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)))\n",
    "    df['digit_count'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df['special_char_count'] = df['text'].apply(lambda x: len(re.findall(r'[^a-zA-Z0-9\\s]', x)))\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    df['unique_char_ratio'] = df['text'].apply(lambda x: len(set(x)) / len(x) if len(x) > 0 else 0)\n",
    "    df['tweet_hour_bin'] = pd.cut(df['hour'], bins=4, labels=[0,1,2,3])\n",
    "    df['tweet_day_bin'] = pd.cut(df['day_of_week'], bins=3, labels=[0,1,2])\n",
    "    df['text_complexity'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    df['char_count'] = df['text'].apply(len)\n",
    "    df['word_density'] = df['text'].apply(lambda x: len(x.split()) / (len(x) + 1))\n",
    "    df['punctuation_ratio'] = df['punctuation_count'] / df['char_count']\n",
    "    df['url_to_text_ratio'] = df['url_count'] / df['text_length']\n",
    "    df['hashtag_to_text_ratio'] = df['hashtag_count'] / df['text_length']\n",
    "    df['mention_to_text_ratio'] = df['mention_count'] / df['text_length']\n",
    "    return df\n",
    "\n",
    "def get_bert_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "class XGBoostProgressCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, display_interval=100, total_iterations=1000):\n",
    "        self.display_interval = display_interval\n",
    "        self.total_iterations = total_iterations\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if (epoch + 1) % self.display_interval == 0 or (epoch + 1) == self.total_iterations:\n",
    "            print(f'XGBoost progress: {epoch + 1}/{self.total_iterations} iterations')\n",
    "        return False\n",
    "\n",
    "print(\"データの読み込みと前処理を開始します...\")\n",
    "df = pd.read_csv('twibot_tweet0_new.csv')\n",
    "df = extract_features(df)\n",
    "print(\"前処理が完了しました。\")\n",
    "\n",
    "X_text = df['text']\n",
    "numeric_features = [col for col in df.columns if col not in ['text', 'label', 'author_id', 'created_at', 'in_reply_to_user_id']]\n",
    "X_numeric = df[numeric_features]\n",
    "y = df['label'].map({'human': 0, 'bot': 1})\n",
    "\n",
    "X_train_text, X_test_text, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_text, X_numeric, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=35000, ngram_range=(1, 3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric)\n",
    "\n",
    "print(\"BERTモデルを使用して文章の埋め込みを生成します...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "X_train_bert = get_bert_embeddings(X_train_text, model, tokenizer)\n",
    "X_test_bert = get_bert_embeddings(X_test_text, model, tokenizer)\n",
    "\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_numeric_scaled, X_train_bert])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_numeric_scaled, X_test_bert])\n",
    "\n",
    "variance_threshold = VarianceThreshold(threshold=0.0005)\n",
    "X_train_var_selected = variance_threshold.fit_transform(X_train_combined)\n",
    "X_test_var_selected = variance_threshold.transform(X_test_combined)\n",
    "\n",
    "selector = SelectKBest(f_classif, k=\"all\")\n",
    "X_train_selected = selector.fit_transform(X_train_var_selected, y_train)\n",
    "X_test_selected = selector.transform(X_test_var_selected)\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.8, random_state=42)\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "steps = [('over', smote), ('under', enn)]\n",
    "pipeline = ImbPipeline(steps=steps)\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=2500, max_depth=60, min_samples_split=3, min_samples_leaf=1, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=2500, learning_rate=0.01, max_depth=12, min_child_weight=1, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=2, random_state=42, callbacks=[XGBoostProgressCallback(display_interval=250, total_iterations=2500)], n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=800, learning_rate=0.03, max_depth=10, min_samples_split=3, min_samples_leaf=1, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', C=150, gamma='scale', probability=True, class_weight='balanced', random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(500, 250, 125), max_iter=2500, alpha=0.00005, learning_rate='adaptive', random_state=42)\n",
    "}\n",
    "\n",
    "print(\"モデルの学習を開始します...\")\n",
    "for name, model in tqdm(models.items()):\n",
    "    print(f\"{name}の学習を開始します...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time.time()\n",
    "    print(f\"{name}の学習が完了しました。学習時間: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "print(\"アンサンブル予測を行います...\")\n",
    "y_pred_rf = models['RandomForest'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_xgb = models['XGBoost'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_gb = models['GradientBoosting'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_svm = models['SVM'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_mlp = models['MLP'].predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "y_pred_ensemble = 0.3 * y_pred_rf + 0.3 * y_pred_xgb + 0.2 * y_pred_gb + 0.1 * y_pred_svm + 0.1 * y_pred_mlp\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "    f1_scores = [f1_score(y_true, (y_pred_proba > t).astype(int)) for t in thresholds]\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "optimal_threshold = find_optimal_threshold(y_test, y_pred_ensemble)\n",
    "y_pred_final = (y_pred_ensemble > optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"最適な閾値: {optimal_threshold}\")\n",
    "print(\"最終的な予測結果:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['human', 'bot']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f3408-1a9b-4724-8688-e183e74adcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データの読み込みと前処理を開始します...\n",
      "前処理が完了しました。\n",
      "BERTモデルを使用して文章の埋め込みを生成します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 24000/24000 [21:44<00:00, 18.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 6000/6000 [05:35<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▏                                                                            | 1/5 [09:01<36:06, 541.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習が完了しました。学習時間: 541.67秒\n",
      "XGBoostの学習を開始します...\n",
      "XGBoost progress: 300/3000 iterations\n",
      "XGBoost progress: 600/3000 iterations\n",
      "XGBoost progress: 900/3000 iterations\n",
      "XGBoost progress: 1200/3000 iterations\n",
      "XGBoost progress: 1500/3000 iterations\n",
      "XGBoost progress: 1800/3000 iterations\n",
      "XGBoost progress: 2100/3000 iterations\n",
      "XGBoost progress: 2400/3000 iterations\n",
      "XGBoost progress: 2700/3000 iterations\n",
      "XGBoost progress: 3000/3000 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████▍                                                         | 2/5 [25:25<40:06, 802.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostの学習が完了しました。学習時間: 984.29秒\n",
      "GradientBoostingの学習を開始します...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "def extract_features(df):\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['has_url'] = df['text'].str.contains('http').astype(int)\n",
    "    df['hour'] = pd.to_datetime(df['created_at']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['created_at']).dt.dayofweek\n",
    "    df['is_reply'] = df['in_reply_to_user_id'].notna().astype(int)\n",
    "    df['hashtag_count'] = df['text'].apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
    "    df['mention_count'] = df['text'].apply(lambda x: len(re.findall(r'@\\w+', x)))\n",
    "    df['retweet_like_ratio'] = df['retweet_count'] / (df['like_count'] + 1)\n",
    "    df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    df['caps_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "    df['unique_words_ratio'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    df['tweet_frequency'] = df.groupby('author_id')['created_at'].transform('count')\n",
    "    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: sum(1 for c in x if c in '.,;:!?'))\n",
    "    df['emoji_count'] = df['text'].apply(lambda x: len(re.findall(r'[\\U0001F600-\\U0001F64F]', x)))\n",
    "    df['exclamation_count'] = df['text'].apply(lambda x: x.count('!'))\n",
    "    df['question_count'] = df['text'].apply(lambda x: x.count('?'))\n",
    "    df['uppercase_word_count'] = df['text'].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "    df['url_count'] = df['text'].apply(lambda x: len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)))\n",
    "    df['digit_count'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df['special_char_count'] = df['text'].apply(lambda x: len(re.findall(r'[^a-zA-Z0-9\\s]', x)))\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    df['unique_char_ratio'] = df['text'].apply(lambda x: len(set(x)) / len(x) if len(x) > 0 else 0)\n",
    "    df['tweet_hour_bin'] = pd.cut(df['hour'], bins=4, labels=[0,1,2,3])\n",
    "    df['tweet_day_bin'] = pd.cut(df['day_of_week'], bins=3, labels=[0,1,2])\n",
    "    df['text_complexity'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    df['char_count'] = df['text'].apply(len)\n",
    "    df['word_density'] = df['text'].apply(lambda x: len(x.split()) / (len(x) + 1))\n",
    "    df['punctuation_ratio'] = df['punctuation_count'] / df['char_count']\n",
    "    df['url_to_text_ratio'] = df['url_count'] / df['text_length']\n",
    "    df['hashtag_to_text_ratio'] = df['hashtag_count'] / df['text_length']\n",
    "    df['mention_to_text_ratio'] = df['mention_count'] / df['text_length']\n",
    "    df['tweet_frequency_per_day'] = df.groupby(['author_id', 'day_of_week'])['created_at'].transform('count')\n",
    "    df['retweet_ratio'] = df['retweet_count'] / (df['retweet_count'] + df['like_count'] + 1)\n",
    "    return df\n",
    "\n",
    "def get_bert_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "class XGBoostProgressCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, display_interval=100, total_iterations=1000):\n",
    "        self.display_interval = display_interval\n",
    "        self.total_iterations = total_iterations\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if (epoch + 1) % self.display_interval == 0 or (epoch + 1) == self.total_iterations:\n",
    "            print(f'XGBoost progress: {epoch + 1}/{self.total_iterations} iterations')\n",
    "        return False\n",
    "\n",
    "print(\"データの読み込みと前処理を開始します...\")\n",
    "df = pd.read_csv('twidata.csv')\n",
    "df = extract_features(df)\n",
    "print(\"前処理が完了しました。\")\n",
    "\n",
    "X_text = df['text']\n",
    "numeric_features = [col for col in df.columns if col not in ['text', 'label', 'author_id', 'created_at', 'in_reply_to_user_id']]\n",
    "X_numeric = df[numeric_features]\n",
    "y = df['label'].map({'human': 0, 'bot': 1})\n",
    "\n",
    "X_train_text, X_test_text, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_text, X_numeric, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=40000, ngram_range=(1, 3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric)\n",
    "\n",
    "print(\"BERTモデルを使用して文章の埋め込みを生成します...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "X_train_bert = get_bert_embeddings(X_train_text, model, tokenizer)\n",
    "X_test_bert = get_bert_embeddings(X_test_text, model, tokenizer)\n",
    "\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_numeric_scaled, X_train_bert])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_numeric_scaled, X_test_bert])\n",
    "\n",
    "variance_threshold = VarianceThreshold(threshold=0.0001)\n",
    "X_train_var_selected = variance_threshold.fit_transform(X_train_combined)\n",
    "X_test_var_selected = variance_threshold.transform(X_test_combined)\n",
    "\n",
    "selector = SelectKBest(f_classif, k=\"all\")\n",
    "X_train_selected = selector.fit_transform(X_train_var_selected, y_train)\n",
    "X_test_selected = selector.transform(X_test_var_selected)\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.7, random_state=42)\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "steps = [('over', smote), ('under', enn)]\n",
    "pipeline = ImbPipeline(steps=steps)\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=3000, max_depth=70, min_samples_split=2, min_samples_leaf=1, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=3000, learning_rate=0.01, max_depth=15, min_child_weight=1, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=3, random_state=42, callbacks=[XGBoostProgressCallback(display_interval=300, total_iterations=3000)], n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=1000, learning_rate=0.02, max_depth=12, min_samples_split=2, min_samples_leaf=1, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', C=200, gamma='scale', probability=True, class_weight='balanced', random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(600, 300, 150), max_iter=3000, alpha=0.00001, learning_rate='adaptive', random_state=42)\n",
    "}\n",
    "\n",
    "print(\"モデルの学習を開始します...\")\n",
    "for name, model in tqdm(models.items()):\n",
    "    print(f\"{name}の学習を開始します...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time.time()\n",
    "    print(f\"{name}の学習が完了しました。学習時間: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "print(\"アンサンブル予測を行います...\")\n",
    "y_pred_rf = models['RandomForest'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_xgb = models['XGBoost'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_gb = models['GradientBoosting'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_svm = models['SVM'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_mlp = models['MLP'].predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "y_pred_ensemble = 0.3 * y_pred_rf + 0.3 * y_pred_xgb + 0.2 * y_pred_gb + 0.1 * y_pred_svm + 0.1 * y_pred_mlp\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.005)\n",
    "    f1_scores = [f1_score(y_true, (y_pred_proba > t).astype(int)) for t in thresholds]\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "optimal_threshold = find_optimal_threshold(y_test, y_pred_ensemble)\n",
    "y_pred_final = (y_pred_ensemble > optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"最適な閾値: {optimal_threshold}\")\n",
    "print(\"最終的な予測結果:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['human', 'bot']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd39dc-ed71-4ea7-8f77-862740c13ceb",
   "metadata": {},
   "source": [
    "# 以下はtwidata.csvを用いた学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5c000a8-c0da-496c-897d-53cbeaaea3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データの読み込みと前処理を開始します...\n",
      "前処理が完了しました。\n",
      "BERTモデルを使用して文章の埋め込みを生成します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 27000/27000 [25:33<00:00, 17.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [02:38<00:00, 18.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▏                                                                            | 1/5 [07:52<31:30, 472.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestの学習が完了しました。学習時間: 472.61秒\n",
      "XGBoostの学習を開始します...\n",
      "XGBoost progress: 200/2000 iterations\n",
      "XGBoost progress: 400/2000 iterations\n",
      "XGBoost progress: 600/2000 iterations\n",
      "XGBoost progress: 800/2000 iterations\n",
      "XGBoost progress: 1000/2000 iterations\n",
      "XGBoost progress: 1200/2000 iterations\n",
      "XGBoost progress: 1400/2000 iterations\n",
      "XGBoost progress: 1600/2000 iterations\n",
      "XGBoost progress: 1800/2000 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████▍                                                         | 2/5 [12:17<17:30, 350.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost progress: 2000/2000 iterations\n",
      "XGBoostの学習が完了しました。学習時間: 264.49秒\n",
      "GradientBoostingの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████▌                                    | 3/5 [3:13:43<2:52:02, 5161.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingの学習が完了しました。学習時間: 10886.26秒\n",
      "SVMの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 4/5 [3:21:25<55:06, 3306.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMの学習が完了しました。学習時間: 462.10秒\n",
      "MLPの学習を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [3:23:04<00:00, 2436.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPの学習が完了しました。学習時間: 98.60秒\n",
      "アンサンブル予測を行います...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適な閾値: 0.8399999999999996\n",
      "最終的な予測結果:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.95      0.84      0.89      2795\n",
      "         bot       0.17      0.44      0.24       205\n",
      "\n",
      "    accuracy                           0.81      3000\n",
      "   macro avg       0.56      0.64      0.57      3000\n",
      "weighted avg       0.90      0.81      0.85      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "def extract_features(df):\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['has_url'] = df['text'].str.contains('http').astype(int)\n",
    "    df['hour'] = pd.to_datetime(df['created_at']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['created_at']).dt.dayofweek\n",
    "    df['is_reply'] = df['in_reply_to_user_id'].notna().astype(int)\n",
    "    df['hashtag_count'] = df['text'].apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
    "    df['mention_count'] = df['text'].apply(lambda x: len(re.findall(r'@\\w+', x)))\n",
    "    df['retweet_like_ratio'] = df['retweet_count'] / (df['like_count'] + 1)\n",
    "    df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    df['caps_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "    df['unique_words_ratio'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    df['tweet_frequency'] = df.groupby('author_id')['created_at'].transform('count')\n",
    "    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0)\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: sum(1 for c in x if c in '.,;:!?'))\n",
    "    df['emoji_count'] = df['text'].apply(lambda x: len(re.findall(r'[\\U0001F600-\\U0001F64F]', x)))\n",
    "    df['exclamation_count'] = df['text'].apply(lambda x: x.count('!'))\n",
    "    df['question_count'] = df['text'].apply(lambda x: x.count('?'))\n",
    "    df['uppercase_word_count'] = df['text'].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "    df['url_count'] = df['text'].apply(lambda x: len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)))\n",
    "    df['digit_count'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df['special_char_count'] = df['text'].apply(lambda x: len(re.findall(r'[^a-zA-Z0-9\\s]', x)))\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    df['unique_char_ratio'] = df['text'].apply(lambda x: len(set(x)) / len(x) if len(x) > 0 else 0)\n",
    "    df['tweet_hour_bin'] = pd.cut(df['hour'], bins=4, labels=[0,1,2,3])\n",
    "    df['tweet_day_bin'] = pd.cut(df['day_of_week'], bins=3, labels=[0,1,2])\n",
    "    df['text_complexity'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
    "    return df\n",
    "\n",
    "def get_bert_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "class XGBoostProgressCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, display_interval=100, total_iterations=1000):\n",
    "        self.display_interval = display_interval\n",
    "        self.total_iterations = total_iterations\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if (epoch + 1) % self.display_interval == 0 or (epoch + 1) == self.total_iterations:\n",
    "            print(f'XGBoost progress: {epoch + 1}/{self.total_iterations} iterations')\n",
    "        return False\n",
    "\n",
    "print(\"データの読み込みと前処理を開始します...\")\n",
    "df = pd.read_csv('twidata.csv')\n",
    "df = extract_features(df)\n",
    "print(\"前処理が完了しました。\")\n",
    "\n",
    "X_text = df['text']\n",
    "numeric_features = [col for col in df.columns if col not in ['text', 'label', 'author_id', 'created_at', 'in_reply_to_user_id']]\n",
    "X_numeric = df[numeric_features]\n",
    "y = df['label'].map({'human': 0, 'bot': 1})\n",
    "\n",
    "# データ分割比率を調整\n",
    "X_train_text, X_test_text, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_text, X_numeric, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=35000, ngram_range=(1, 3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric)\n",
    "\n",
    "print(\"BERTモデルを使用して文章の埋め込みを生成します...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "X_train_bert = get_bert_embeddings(X_train_text, model, tokenizer)\n",
    "X_test_bert = get_bert_embeddings(X_test_text, model, tokenizer)\n",
    "\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_numeric_scaled, X_train_bert])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_numeric_scaled, X_test_bert])\n",
    "\n",
    "variance_threshold = VarianceThreshold(threshold=0.001)\n",
    "X_train_var_selected = variance_threshold.fit_transform(X_train_combined)\n",
    "X_test_var_selected = variance_threshold.transform(X_test_combined)\n",
    "\n",
    "selector = SelectKBest(f_classif, k=\"all\")\n",
    "X_train_selected = selector.fit_transform(X_train_var_selected, y_train)\n",
    "X_test_selected = selector.transform(X_test_var_selected)\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.7, random_state=42)\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "steps = [('over', smote), ('under', enn)]\n",
    "pipeline = ImbPipeline(steps=steps)\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=2000, max_depth=50, min_samples_split=4, min_samples_leaf=2, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=2000, learning_rate=0.02, max_depth=10, min_child_weight=1, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=2, random_state=42, callbacks=[XGBoostProgressCallback(display_interval=200, total_iterations=2000)], n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=500, learning_rate=0.05, max_depth=8, min_samples_split=4, min_samples_leaf=2, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', C=100, gamma='scale', probability=True, class_weight='balanced', random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(400, 200, 100), max_iter=2000, alpha=0.0001, learning_rate='adaptive', random_state=42)\n",
    "}\n",
    "\n",
    "print(\"モデルの学習を開始します...\")\n",
    "for name, model in tqdm(models.items()):\n",
    "    print(f\"{name}の学習を開始します...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    end_time = time.time()\n",
    "    print(f\"{name}の学習が完了しました。学習時間: {end_time - start_time:.2f}秒\")\n",
    "\n",
    "print(\"アンサンブル予測を行います...\")\n",
    "y_pred_rf = models['RandomForest'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_xgb = models['XGBoost'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_gb = models['GradientBoosting'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_svm = models['SVM'].predict_proba(X_test_selected)[:, 1]\n",
    "y_pred_mlp = models['MLP'].predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "y_pred_ensemble = 0.3 * y_pred_rf + 0.3 * y_pred_xgb + 0.2 * y_pred_gb + 0.1 * y_pred_svm + 0.1 * y_pred_mlp\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "    f1_scores = [f1_score(y_true, (y_pred_proba > t).astype(int)) for t in thresholds]\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "optimal_threshold = find_optimal_threshold(y_test, y_pred_ensemble)\n",
    "y_pred_final = (y_pred_ensemble > optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"最適な閾値: {optimal_threshold}\")\n",
    "print(\"最終的な予測結果:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['human', 'bot']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f16b3-6d50-4da1-a774-7625635db3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
